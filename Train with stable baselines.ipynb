{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with stable baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_defender\n",
    "\n",
    "from stable_baselines.common.policies import FeedForwardPolicy as FFP_common\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy as FFP_DQ\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import DQN\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_PPO(FFP_common):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MLP_PPO, self).__init__(*args, **kwargs,\n",
    "                                           net_arch=[dict(pi=[300, 300], vf=[300, 300])],\n",
    "                                           feature_extraction=\"mlp\")\n",
    "        \n",
    "class MLP_DQN(FFP_DQ):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MLP_DQN, self).__init__(*args, **kwargs,\n",
    "                                           layers=[300, 300],\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defense:\n",
    "    def __init__(self, method, K=5, P=0.95):\n",
    "        self.method = method\n",
    "        \n",
    "        self.K = K\n",
    "        self.state_size        = 2 * (self.K + 1)\n",
    "        self.action_size       = 2\n",
    "        self.reward            = []\n",
    "        \n",
    "        env_name = 'defender-' + str(K) + '-' + str(P) + '-v1'\n",
    "        \n",
    "        env = gym.make(env_name)\n",
    "        self.envs = DummyVecEnv([lambda: env])\n",
    "        \n",
    "        if method=='PPO':\n",
    "            self.model = PPO2(MLP_PPO, self.envs, verbose=0)\n",
    "        elif method=='DQN':\n",
    "            self.model = DQN(MLP_DQN, self.envs, verbose=0)\n",
    "        else:\n",
    "            raise Exception(\"Erreur ! MÃ©thode: 'PPO' ou 'DQN'\")\n",
    "        \n",
    "        \n",
    "    def learn(self, timesteps=10000):\n",
    "        self.model.learn(total_timesteps=timesteps)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        if self.method=='PPO':\n",
    "            self.model = PPO2.load(filename, policy=MLP_PPO)\n",
    "        else:\n",
    "            self.model = DQN.load(filename, policy=MLP_DQN)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def run(self, nb_episodes = 1000):\n",
    "        self.nb_episodes = nb_episodes\n",
    "        \n",
    "        for index_episode in range(nb_episodes):\n",
    "            state = self.envs.reset()\n",
    "            state = np.reshape(np.array(state), [1, self.state_size])\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                 action, _states = model.predict(state)\n",
    "                 next_state, reward, done, _ = self.envs.step(action)\n",
    "                 next_state = np.reshape(np.array(next_state), [1, self.state_size])\n",
    "                 state = next_state\n",
    "                 steps += 1\n",
    "            if index_episode %100 == 0:\n",
    "                print(\"Episode {}#; \\t Nb of steps: {}; \\t Reward: {}.\".format(index_episode, steps + 1, reward))\n",
    "            if index_episode > 0:\n",
    "                self.reward += [((self.reward[-1] * len(self.reward)) + reward) / (len(self.reward) + 1)]\n",
    "            else:\n",
    "                self.reward += [reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defense = Defense('DQN', K=5, P=0.95)\n",
    "defense.learn(20000)\n",
    "print(\"======\\nLEARNING DONE\\n======\")\n",
    "defense.run(2000)\n",
    "plot(range(1, defense.nb_episodes+1), defense.reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
